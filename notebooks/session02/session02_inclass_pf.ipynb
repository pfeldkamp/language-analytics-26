{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b960cf5d45a879",
   "metadata": {},
   "source": [
    "# Text as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a416f6-f9bf-4ea2-8452-ff1a2332b181",
   "metadata": {},
   "source": [
    "## Segmentation and tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e646be-bb26-4621-9f36-894ca7f39e80",
   "metadata": {},
   "source": [
    "In this section we will try to chop up üó°Ô∏è a piece of text into words (tokens) that we can count and analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf7dce8-6588-4ba8-9a49-675043cd720e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.288790Z",
     "start_time": "2025-05-20T10:24:08.286414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example string. Text data is not trivial to work with, even if it is nice and clean.\n",
      "\n",
      "Things like punctuation and abbreviations, e.g. 'cf.' and 'i.e.', spell trouble for simple approaches of segmentation and tokenization.\n"
     ]
    }
   ],
   "source": [
    "example_string = \"This is an example string. Text data is not trivial to work with, even if it is nice and clean.\\n\\nThings like punctuation and abbreviations, e.g. 'cf.' and 'i.e.', spell trouble for simple approaches of segmentation and tokenization.\"\n",
    "print(example_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b112fe-f2e4-40bf-a449-65dcd3bdaa87",
   "metadata": {},
   "source": [
    "A naive approach of splitting a text periods for sentences and on whitespace gets us pretty far, but as we shall see, will not get us all the way. Let's try out a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146078b2-41d4-4491-9b91-bfc2b8416175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.317909Z",
     "start_time": "2025-05-20T10:24:08.315942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is an example string',\n",
       " ' Text data is not trivial to work with, even if it is nice and clean',\n",
       " '\\n\\nThings like punctuation and abbreviations, e',\n",
       " 'g',\n",
       " \" 'cf\",\n",
       " \"' and 'i\",\n",
       " 'e',\n",
       " \"', spell trouble for simple approaches of segmentation and tokenization\",\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_string.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50c01f-de16-46c8-94ed-851fbb4cad3f",
   "metadata": {},
   "source": [
    "We mostly get complete sentences, but the abbreviations cause trouble. Let's forget sentences for now and focus on individual words. We start by splitting on whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874028b8-7a10-45a3-a5c2-66ceb78a1b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.331569Z",
     "start_time": "2025-05-20T10:24:08.329739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string.',\n",
       " 'Text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with,',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean.\\n\\nThings']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_string.split(' ')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4725a93-8ef8-44dd-a768-48525afb6231",
   "metadata": {},
   "source": [
    "Hmm, the period sticks to the `string.` token and the comma to the `with,` token. Let's try a regex which will split on (most) punctuation and a whitespace by defining a group of punctuation characters that _may_ (see the use of the `?` operator) precede a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43390ddf-e2bd-43f6-b1f3-1a77dac4fcc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.344051Z",
     "start_time": "2025-05-20T10:24:08.342493Z"
    }
   },
   "outputs": [],
   "source": [
    "import re # regular expression module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4074b65-9bbb-4c8b-a36f-a7cd20b9ff2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.351511Z",
     "start_time": "2025-05-20T10:24:08.349553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string',\n",
       " 'Text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean.\\n\\nThings',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'abbreviations',\n",
       " 'e.g',\n",
       " \"'cf.'\",\n",
       " 'and',\n",
       " \"'i.e.'\",\n",
       " 'spell',\n",
       " 'trouble']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = '[.,;?!]? '\n",
    "regex = re.compile(pattern)\n",
    "regex.split(example_string, )[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30018170-eddc-42a5-9da4-f58d1c169172",
   "metadata": {},
   "source": [
    "Better. But we have some newlines (`\\n`) causing trouble. We can use the whitespace `\\s` and say that there should be one or more of them by using the `+` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d44e9c1f-31c0-4551-9774-6d320f8f241f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.365968Z",
     "start_time": "2025-05-20T10:24:08.363794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string',\n",
       " 'Text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean',\n",
       " 'Things',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'abbreviations',\n",
       " 'e.g',\n",
       " \"'cf.'\",\n",
       " 'and',\n",
       " \"'i.e.'\",\n",
       " 'spell']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'[.,;?!]?\\s+'\n",
    "regex = re.compile(pattern)\n",
    "regex.split(example_string)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9ad97-3aa9-4555-aacd-066b1fd6f368",
   "metadata": {},
   "source": [
    "Hmm, notice that we also lost the final period in `e.g.`, resulting in a truncated `e.g` token. We could handle the different abbreviations in our regex. Also, we may actually want to keep the punctuation as tokens themselves, but probably not the whitespaces since they are not that interesting.\n",
    "\n",
    "You can probably see where this is going. To handle all cases that do not conform to a simple \"split on whitespace\" approach, the regex grows more complex.\n",
    "\n",
    "Instead of reinventing the wheel, we will use tokenizers from the Natural Language Toolkit for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89175fa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.931846Z",
     "start_time": "2025-05-20T10:24:08.372490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk<=3.8.1\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /Users/au324704/.pyenv/versions/3.12.11/lib/python3.12/site-packages (from nltk<=3.8.1) (8.3.1)\n",
      "Requirement already satisfied: joblib in /Users/au324704/.pyenv/versions/3.12.11/lib/python3.12/site-packages (from nltk<=3.8.1) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/au324704/.pyenv/versions/3.12.11/lib/python3.12/site-packages (from nltk<=3.8.1) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /Users/au324704/.pyenv/versions/3.12.11/lib/python3.12/site-packages (from nltk<=3.8.1) (4.67.1)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"nltk<=3.8.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62ab366-83e4-4273-b7e7-f81773cfee62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.940905Z",
     "start_time": "2025-05-20T10:24:08.938284Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/au324704/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import ssl\n",
    "\n",
    "# See: https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt') # underlying model for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1896bd91-7226-47eb-b11a-c39fe5aa53e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.957119Z",
     "start_time": "2025-05-20T10:24:08.948410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string',\n",
       " '.',\n",
       " 'Text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " ',',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean',\n",
       " '.',\n",
       " 'Things',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'abbreviations',\n",
       " ',',\n",
       " 'e.g']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(example_string)\n",
    "tokens[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fd2fa-8d01-497c-adf7-4b65684c1858",
   "metadata": {},
   "source": [
    "We can count those tokens and see what makes it to the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d927387f-28c9-4e5d-985e-c6786e883b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.964919Z",
     "start_time": "2025-05-20T10:24:08.962871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 6),\n",
       " ('and', 4),\n",
       " ('is', 3),\n",
       " (',', 3),\n",
       " (\"'\", 3),\n",
       " ('This', 1),\n",
       " ('an', 1),\n",
       " ('example', 1),\n",
       " ('string', 1),\n",
       " ('Text', 1),\n",
       " ('data', 1),\n",
       " ('not', 1),\n",
       " ('trivial', 1),\n",
       " ('to', 1),\n",
       " ('work', 1),\n",
       " ('with', 1),\n",
       " ('even', 1),\n",
       " ('if', 1),\n",
       " ('it', 1),\n",
       " ('nice', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(tokens)\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b79f4a87-7b27-4cf2-89be-0033b7ee3180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.977581Z",
     "start_time": "2025-05-20T10:24:08.976468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 6,\n",
       " 'and': 4,\n",
       " 'is': 3,\n",
       " ',': 3,\n",
       " \"'\": 3,\n",
       " 'This': 1,\n",
       " 'an': 1,\n",
       " 'example': 1,\n",
       " 'string': 1,\n",
       " 'Text': 1,\n",
       " 'data': 1,\n",
       " 'not': 1,\n",
       " 'trivial': 1,\n",
       " 'to': 1,\n",
       " 'work': 1,\n",
       " 'with': 1,\n",
       " 'even': 1,\n",
       " 'if': 1,\n",
       " 'it': 1,\n",
       " 'nice': 1,\n",
       " 'clean': 1,\n",
       " 'Things': 1,\n",
       " 'like': 1,\n",
       " 'punctuation': 1,\n",
       " 'abbreviations': 1,\n",
       " 'e.g': 1,\n",
       " \"'cf\": 1,\n",
       " 'i.e': 1,\n",
       " 'spell': 1,\n",
       " 'trouble': 1,\n",
       " 'for': 1,\n",
       " 'simple': 1,\n",
       " 'approaches': 1,\n",
       " 'of': 1,\n",
       " 'segmentation': 1,\n",
       " 'tokenization': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is basically the same as looping through and assigning counts\n",
    "word_counts = {}\n",
    "for token in tokens:\n",
    "    if token in word_counts:\n",
    "        word_counts[token] += 1\n",
    "    else:\n",
    "        word_counts[token] = 1\n",
    "\n",
    "# sorting the dictionary by values\n",
    "dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cfcf4",
   "metadata": {},
   "source": [
    "## ALRIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dd14edd-148c-469a-ac2e-2364798bf05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.987751Z",
     "start_time": "2025-05-20T10:24:08.986003Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "for filename in glob('../gutenberg/*.txt'):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f1689-ffd3-4fec-8574-39e170b63328",
   "metadata": {},
   "source": [
    "## Loading some text data\n",
    "Let's get some simple text data to work with. It does not matter much exactly what - just that it is plain text data.\n",
    "\n",
    "A good place to start is [Project Gutenberg](https://www.gutenberg.org/) which has free e-books available in plain text files. I have downloaded the top books and stored them in the `data` folder.\n",
    "\n",
    "You can try to find other text sources and load them in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f5919-8660-453b-bf74-fe4654223486",
   "metadata": {},
   "source": [
    "Let's try to work with the text data from _Moby Dick_. Load the text, and chop it into chapters with a regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f3e55d5-cf67-460d-b124-bf926736144d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:44.284611Z",
     "start_time": "2025-05-20T10:24:44.261262Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/gutenberg/moby_dick.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m chapter_splitter = re.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn\u001b[39m\u001b[33m{\u001b[39m\u001b[33m3,}CHAPTER \u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m. \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../../data/gutenberg/moby_dick.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     raw_text = f.read()\n\u001b[32m      5\u001b[39m     chapters = chapter_splitter.split(raw_text)[\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m] \u001b[38;5;66;03m# removing title, preface etc. up to chapter 1 by slicing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../data/gutenberg/moby_dick.txt'"
     ]
    }
   ],
   "source": [
    "chapter_splitter = re.compile(r'\\n{3,}CHAPTER \\d+\\. ')\n",
    "\n",
    "with open('../../data/gutenberg/moby_dick.txt') as f:\n",
    "    raw_text = f.read()\n",
    "    chapters = chapter_splitter.split(raw_text)[1:-1] # removing title, preface etc. up to chapter 1 by slicing\n",
    "\n",
    "for chapter in chapters[:10]:\n",
    "    print(chapter[:100] + ' ...')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693f527-830b-4964-8bbb-a2d6d1edae22",
   "metadata": {},
   "source": [
    "Let's count some words from the first chapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32d01b31-1f45-491f-a39e-74bff66391c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:45.605938Z",
     "start_time": "2025-05-20T10:24:45.599995Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 169),\n",
       " ('the', 121),\n",
       " ('of', 81),\n",
       " ('.', 79),\n",
       " ('a', 68),\n",
       " ('and', 66),\n",
       " ('to', 53),\n",
       " ('in', 46),\n",
       " ('I', 43),\n",
       " ('is', 34),\n",
       " ('that', 31),\n",
       " ('it', 26),\n",
       " ('as', 26),\n",
       " (';', 25),\n",
       " ('me', 24),\n",
       " ('all', 23),\n",
       " ('you', 23),\n",
       " ('?', 18),\n",
       " ('this', 16),\n",
       " ('my', 14)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_chapter = chapters[0] # remember zero indexing!\n",
    "tokens = word_tokenize(first_chapter)\n",
    "counter = Counter(tokens)\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99d7b6-66ce-4415-bcaa-750dde6ff744",
   "metadata": {},
   "source": [
    "This is a bit more interesting than the two or one word counts from the example sentence above. But it still does not tell us much since  the top ranking words are just punctuation and function words like _the_ and _of_. Let's do some filtering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06fbc0-2eb1-4234-b9c8-a5c6b2805c09",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba113403-f4ab-47d5-af57-354d59619eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:46.768340Z",
     "start_time": "2025-05-20T10:24:46.596662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/au479461/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from nltk import corpus\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8a08692-87eb-447c-94bd-f8c06e854245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:46.818124Z",
     "start_time": "2025-05-20T10:24:46.814255Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuation_set = set(punctuation)\n",
    "stopwords_set = set(corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de5c7290-c2e9-4414-bc18-759d49e9fc2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:47.011965Z",
     "start_time": "2025-05-20T10:24:47.010093Z"
    }
   },
   "outputs": [],
   "source": [
    "# short-hand way with list comprehension:\n",
    "# filtered_tokens = [token for token in tokens if token not in punctuation_set and token not in stopwords_set]\n",
    "\n",
    "filtered_tokens = []\n",
    "for token in tokens:\n",
    "    if token not in punctuation_set and token not in stopwords_set:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7efc3fb5-1e40-4598-8aba-57e90fe7ea75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:47.265010Z",
     "start_time": "2025-05-20T10:24:47.262622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 43),\n",
       " ('sea', 10),\n",
       " ('one', 10),\n",
       " ('go', 10),\n",
       " ('upon', 9),\n",
       " ('But', 8),\n",
       " ('part', 7),\n",
       " ('‚Äô', 7),\n",
       " ('And', 7),\n",
       " ('see', 6),\n",
       " ('It', 6),\n",
       " ('get', 6),\n",
       " ('time', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('old', 6),\n",
       " ('take', 5),\n",
       " ('What', 5),\n",
       " ('ever', 5)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_counter = Counter(filtered_tokens)\n",
    "filtered_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bdb10-f978-49b3-8fd9-19135d671a9f",
   "metadata": {},
   "source": [
    "Better. But this reveals an issue that we need to fix: some of the stopwords are not removed because they begin with a capital letter, e.g. _But_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6897b-dffe-4385-9e9e-d1e95e741455",
   "metadata": {},
   "source": [
    "We can also make other observations.\n",
    "\n",
    "The first is that capitalized and non-capitalized versions of a word (e.g. _Tell_ vs _tell_) are counted as two different words. Depending on our analysis, we may want to count them as one.\n",
    "\n",
    "The second is that inflected and non-inflected versions of a word (e.g. _passenger_ vs _passengers_) are counted as different words. Again, depending on our analysis, we may want to count them as one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca5554-8b74-4b9a-90c5-b6b3443825c3",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817c75f-435f-4313-b01f-bd44ccddf621",
   "metadata": {},
   "source": [
    "Normalization is about handling differences. This includes things like:\n",
    "1. Capitalization, e.g. _Tell_ vs _tell_.\n",
    "2. UK and US spelling, e.g. _colour_ vs. _color_. (We will skip that here; it is mostly relevant across different texts)\n",
    "3. Inflection, e.g. _passenger_ vs _passengers_.\n",
    "\n",
    "Be mindful that each such a normalization process eliminate potentially meaningful information. That is, there are reasons for capitalization; consider _apple_ vs _Apple_ or _bill_ vs _Bill_, and plural inflection or past tense are there for a reason.\n",
    "\n",
    "For this exercise we will do decapitalization and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb392e1d-8625-421d-939d-f148aad20a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:48.714567Z",
     "start_time": "2025-05-20T10:24:48.711876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loomings',\n",
       " 'call',\n",
       " 'ishmael',\n",
       " 'years',\n",
       " 'ago‚Äînever',\n",
       " 'mind',\n",
       " 'long',\n",
       " 'precisely‚Äîhaving',\n",
       " 'little',\n",
       " 'money',\n",
       " 'purse',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'interest',\n",
       " 'shore',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'little',\n",
       " 'see',\n",
       " 'watery',\n",
       " 'part',\n",
       " 'world',\n",
       " 'way',\n",
       " 'driving',\n",
       " 'spleen',\n",
       " 'regulating',\n",
       " 'circulation',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'growing',\n",
       " 'grim',\n",
       " 'mouth',\n",
       " 'whenever',\n",
       " 'damp',\n",
       " 'drizzly',\n",
       " 'november',\n",
       " 'soul',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'involuntarily',\n",
       " 'pausing',\n",
       " 'coffin',\n",
       " 'warehouses',\n",
       " 'bringing',\n",
       " 'rear',\n",
       " 'every',\n",
       " 'funeral',\n",
       " 'meet',\n",
       " 'especially']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# short-hand way with list comprehension\n",
    "# lowercase_tokens = [token.lower() for token in tokens]\n",
    "# filtered_lowercase_tokens = [token for token in lowercase_tokens\n",
    "#                              if token not in punctuation_set and token not in stopwords_set]\n",
    "\n",
    "lowercase_tokens = []\n",
    "for token in tokens:\n",
    "    lowercase_tokens.append(token.lower())\n",
    "\n",
    "filtered_lowercase_tokens = []\n",
    "for token in lowercase_tokens:\n",
    "    if token not in punctuation_set and token not in stopwords_set:\n",
    "        filtered_lowercase_tokens.append(token)\n",
    "\n",
    "filtered_lowercase_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f79217ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:49.319303Z",
     "start_time": "2025-05-20T10:24:49.316889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 12),\n",
       " ('one', 11),\n",
       " ('sea', 10),\n",
       " ('upon', 9),\n",
       " ('part', 7),\n",
       " ('‚Äô', 7),\n",
       " ('see', 6),\n",
       " ('get', 6),\n",
       " ('time', 6),\n",
       " ('take', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('voyage', 6),\n",
       " ('old', 6),\n",
       " ('whenever', 5),\n",
       " ('ever', 5),\n",
       " ('sailor', 5),\n",
       " ('whaling', 5),\n",
       " ('little', 4)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_lowercase_counter = Counter(filtered_lowercase_tokens)\n",
    "filtered_lowercase_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7696bca-c7e7-48f3-aa6d-c8cd56b512cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:49.593854Z",
     "start_time": "2025-05-20T10:24:49.584903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loom',\n",
       " 'call',\n",
       " 'ishmael',\n",
       " 'year',\n",
       " 'ago‚Äînev',\n",
       " 'mind',\n",
       " 'long',\n",
       " 'precisely‚Äîhav',\n",
       " 'littl',\n",
       " 'money',\n",
       " 'purs',\n",
       " 'noth',\n",
       " 'particular',\n",
       " 'interest',\n",
       " 'shore',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'littl',\n",
       " 'see',\n",
       " 'wateri',\n",
       " 'part',\n",
       " 'world',\n",
       " 'way',\n",
       " 'drive',\n",
       " 'spleen',\n",
       " 'regul',\n",
       " 'circul',\n",
       " 'whenev',\n",
       " 'find',\n",
       " 'grow',\n",
       " 'grim',\n",
       " 'mouth',\n",
       " 'whenev',\n",
       " 'damp',\n",
       " 'drizzli',\n",
       " 'novemb',\n",
       " 'soul',\n",
       " 'whenev',\n",
       " 'find',\n",
       " 'involuntarili',\n",
       " 'paus',\n",
       " 'coffin',\n",
       " 'warehous',\n",
       " 'bring',\n",
       " 'rear',\n",
       " 'everi',\n",
       " 'funer',\n",
       " 'meet',\n",
       " 'especi']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# short-hand way with list comprehension\n",
    "# stemmed_filtered_lowercase_tokens = [stemmer.stem(token) for token in filtered_lowercase_tokens]\n",
    "\n",
    "stemmed_filtered_lowercase_tokens = []\n",
    "for token in filtered_lowercase_tokens:\n",
    "    stemmed = stemmer.stem(token)\n",
    "    stemmed_filtered_lowercase_tokens.append(stemmed)\n",
    "\n",
    "stemmed_filtered_lowercase_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8291d648-4bfe-49a1-8441-eeaf94ba8f53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:49.915071Z",
     "start_time": "2025-05-20T10:24:49.912519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 15),\n",
       " ('sea', 12),\n",
       " ('one', 11),\n",
       " ('part', 10),\n",
       " ('upon', 9),\n",
       " ('whale', 8),\n",
       " ('get', 7),\n",
       " ('‚Äô', 7),\n",
       " ('take', 7),\n",
       " ('passeng', 7),\n",
       " ('see', 6),\n",
       " ('time', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('voyag', 6),\n",
       " ('old', 6),\n",
       " ('thing', 6),\n",
       " ('sailor', 6),\n",
       " ('whenev', 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(stemmed_filtered_lowercase_tokens)\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabd93d-8c83-4abf-9b4a-5626a7cbdc44",
   "metadata": {},
   "source": [
    "### A note on stemming\n",
    "Stemming can be quite aggressive in how much it removes of a word, e.g. _passeng_ from _passenger_ and _passengers_. An alternative is lemmatization which gives the dictionary lookup form of a word, but it requires POS-tags on the tokens to work properly. That is, _passengers_ and _passenger_ would both become _passenger_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73569ce",
   "metadata": {},
   "source": [
    "## Concordances\n",
    "/ Keywords in context (KWIC) concordances are a way to look at the contexts in which a word appears. This can be useful for getting a sense of the meaning of a word, or for finding interesting associations between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define keyword\n",
    "keyword = \"love\"\n",
    "\n",
    "# for every token\n",
    "for idx, token in enumerate(cleaned):\n",
    "    if token == keyword:\n",
    "        before = ' '.join(cleaned[idx-5:idx])\n",
    "        after = ' '.join(cleaned[idx+1:idx+6])\n",
    "        full = [before, token, after]\n",
    "        print(\"{:50} {:20} {:50}\".format(*full))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb3e4b",
   "metadata": {},
   "source": [
    "## Collocations \n",
    "\n",
    "Words that co-occur more often than expected by chance are called collocations. They can be interesting to look at because they may reveal meaningful associations between words, e.g. _white_ and _whale_ in _Moby Dick_\n",
    "\n",
    "We can decide the window ourselves. Shorter windows would mean more link..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fb8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "# Example token list\n",
    "tokens = [\"whale\"]\n",
    "window_default_size = 3\n",
    "\n",
    "# Create finder with window size 3\n",
    "finder = BigramCollocationFinder.from_words(tokens, window_size=window_default_size)\n",
    "\n",
    "# Apply frequency filter if desired\n",
    "finder.apply_freq_filter(1)\n",
    "\n",
    "# Rank by PMI\n",
    "bigrams = finder.nbest(BigramAssocMeasures.pmi, 10)\n",
    "print(bigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
